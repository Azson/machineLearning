{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.special import expit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(object):\n",
    "    \n",
    "    def __init__(self, input_nodes, hidden_nodes, output_nodes, learning_rate):\n",
    "        \n",
    "        self.i_nodes = input_nodes\n",
    "        self.h_nodes = hidden_nodes\n",
    "        self.o_nodes = output_nodes\n",
    "        self.lr = learning_rate\n",
    "        \n",
    "        self.ih_weights = np.random.uniform(.0, pow(self.h_nodes, -0.5), (self.h_nodes, self.i_nodes))\n",
    "        self.ho_weights = np.random.uniform(.0, pow(self.o_nodes, -0.5), (self.o_nodes, self.h_nodes))\n",
    "        \n",
    "        self.activation_func = lambda x: expit(x)\n",
    "        \n",
    "    #forward propagation and back propagation\n",
    "    def train(self, input_list, true_list):\n",
    "        \n",
    "        inputs = np.array(input_list, ndmin=2).T\n",
    "        targets = np.array(true_list, ndmin=2).T\n",
    "        \n",
    "        hidden_inputs = np.dot(self.ih_weights, inputs)\n",
    "        hidden_outputs = self.activation_func(hidden_inputs)\n",
    "        \n",
    "        final_inputs = np.dot(self.ho_weights, hidden_outputs)\n",
    "        final_outputs = self.activation_func(final_inputs)\n",
    "        \n",
    "        #back propagation\n",
    "        output_errors = targets - final_outputs\n",
    "        hidden_errors =  np.dot(self.ho_weights.T, output_errors)\n",
    "        \n",
    "        self.ho_weights += self.lr * np.dot((output_errors * final_outputs * (1.0 - final_outputs)), \\\n",
    "                                            np.transpose(hidden_outputs))\n",
    "        self.ih_weights += self.lr * np.dot((hidden_errors * hidden_outputs * (1.0 - hidden_outputs)), \\\n",
    "                                           np.transpose(inputs))\n",
    "        \n",
    "        \n",
    "        pass\n",
    "    \n",
    "    \n",
    "    def query(self, input_list):\n",
    "        \n",
    "        inputs = np.array(input_list, ndmin=2).T\n",
    "        \n",
    "        hidden_inputs = np.dot(self.ih_weights, inputs)\n",
    "        hidden_outputs = self.activation_func(hidden_inputs)\n",
    "        \n",
    "        final_inputs = np.dot(self.ho_weights, hidden_outputs)\n",
    "        final_outputs = self.activation_func(final_inputs)\n",
    "        \n",
    "        return final_outputs\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_NODES = 3\n",
    "HIDDEN_NODES = 3\n",
    "OUTPUT_NODES = 3\n",
    "learning_rate = 0.5\n",
    "\n",
    "\n",
    "nn = NeuralNetwork(INPUT_NODES, HIDDEN_NODES, OUTPUT_NODES, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.69424106],\n",
       "       [0.73910843],\n",
       "       [0.6636021 ]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.query([1.1, 2.1, 3.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x = [\n",
    "    [1, 0, 0],\n",
    "    [0, 1, 0],\n",
    "    [1, 1, 0],\n",
    "    [0, 0, 0]\n",
    "]\n",
    "\n",
    "test_y = [\n",
    "    [1, 0, 0],\n",
    "    [1, 0, 0],\n",
    "    [0, 1, 0],\n",
    "    [0, 1, 0]\n",
    "]\n",
    "\n",
    "nn.train(test_x, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "need_length = 1000\n",
    "\n",
    "with open('data/mnist_test.csv', 'r') as f:\n",
    "    mnist_y = []\n",
    "    mnist_x = []\n",
    "    for i in range(need_length):\n",
    "        row = f.readline().split(',')\n",
    "        mnist_y.append(row[0])\n",
    "        mnist_x.append(row[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label is 7\n",
      "pic show below:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x130742f28>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "label, image_arr = mnist_y[0], np.array(mnist_x[0], dtype=np.float).reshape((28, 28))\n",
    "\n",
    "print('label is {0}\\npic show below:'.format(label))\n",
    "plt.imshow(image_arr, cmap='Greys', interpolation='None')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_NODES = 28*28\n",
    "HIDDEN_NODES = 100\n",
    "OUTPUT_NODES = 10\n",
    "learning_rate = 0.2\n",
    "\n",
    "EPOCHS = 5\n",
    "\n",
    "train_size = 900\n",
    "test_size = 100\n",
    "\n",
    "mnist_nn = NeuralNetwork(INPUT_NODES, HIDDEN_NODES, OUTPUT_NODES, learning_rate)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    for i in range(train_size):\n",
    "\n",
    "        inputs = np.array(mnist_x[i], dtype=np.float) / 255 * 0.99 + 0.01\n",
    "        label = np.zeros(OUTPUT_NODES) + 0.01\n",
    "        label[int(mnist_y[i])] = 0.99\n",
    "\n",
    "        mnist_nn.train(inputs, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label is 9\n",
      "pic show below:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x130cee048>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "label, image_arr = mnist_y[-1], np.array(mnist_x[-1], dtype=np.float).reshape((28, 28))\n",
    "\n",
    "print('label is {0}\\npic show below:'.format(label))\n",
    "plt.imshow(image_arr, cmap='Greys', interpolation='None')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.01690708],\n",
       "       [0.01756821],\n",
       "       [0.01931665],\n",
       "       [0.01464005],\n",
       "       [0.13822195],\n",
       "       [0.00904636],\n",
       "       [0.01675017],\n",
       "       [0.43178942],\n",
       "       [0.08821457],\n",
       "       [0.46657359]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist_nn.query(np.array(mnist_x[-1], dtype=np.float) / 255 * 0.99 + 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true 3, pred 8\n",
      "true 2, pred 7\n",
      "true 3, pred 8\n",
      "true 3, pred 8\n",
      "true 2, pred 7\n",
      "true 6, pred 0\n",
      "true 4, pred 9\n",
      "true 7, pred 2\n",
      "true 8, pred 9\n",
      "true 3, pred 8\n",
      "true 2, pred 0\n",
      "true 3, pred 5\n",
      "true 2, pred 7\n",
      "true 7, pred 9\n",
      "true 4, pred 9\n",
      "true 4, pred 9\n",
      "error (numbers, rate) : ((16, 0.16))\n"
     ]
    }
   ],
   "source": [
    "def test_score():\n",
    "    err = 0\n",
    "    for i in range(test_size):\n",
    "        target = int(mnist_y[-i])\n",
    "        pred_prob = mnist_nn.query(np.array(mnist_x[-i], dtype=np.float) / 255 * 0.99 + 0.01)\n",
    "        pred = np.argmax(pred_prob)\n",
    "        \n",
    "        if target != pred:\n",
    "            print('true {0}, pred {1}'.format(target, pred))\n",
    "            err += 1\n",
    "    \n",
    "    return err, err/test_size\n",
    "\n",
    "print('error (numbers, rate) : ({0})'.format(test_score()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(X, y, rate=0.3):\n",
    "    \n",
    "    test_idx = np.random.choice(len(y), replace=False)\n",
    "    \n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_accuration(target, pred):\n",
    "    err = 0\n",
    "    sz = len(target)\n",
    "    \n",
    "    for idx in range(sz):\n",
    "        if target[idx] != pred[idx]:\n",
    "            err += 1\n",
    "    \n",
    "    return err, err/sz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_to_arr():\n",
    "    ip_arr = []\n",
    "    out_arr = []\n",
    "    for i in range(train_size+test_size):\n",
    "\n",
    "        inputs = np.array(mnist_x[i], dtype=np.double) / 255 * 0.99 + 0.01\n",
    "        \n",
    "        label = np.zeros(OUTPUT_NODES, dtype=np.double) + 0.01\n",
    "        label[int(mnist_y[i])] = 0.99\n",
    "        \n",
    "        ip_arr.append(inputs)\n",
    "        out_arr.append(label)\n",
    "    \n",
    "    return np.array(ip_arr), np.array(out_arr)\n",
    "\n",
    "\n",
    "data_x, data_y = data_to_arr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error 17, error rate 0.16831683168316833\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "\n",
    "def cmp_skl_knn(train_X, train_y, test_X, test_y):\n",
    "    model = KNeighborsClassifier(n_neighbors=3)\n",
    "    model.fit(train_X, train_y)\n",
    "    \n",
    "    pred_y = model.predict(test_X)\n",
    "    \n",
    "    err, err_rate = cal_accuration(test_y, pred_y)\n",
    "    \n",
    "    print(\"error {0}, error rate {1}\".format(err, err_rate))\n",
    "    \n",
    "    \n",
    "cmp_skl_knn(data_x[:train_size, :], mnist_y[:train_size], data_x[-test_size-1:, :], mnist_y[-test_size-1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pass anotation\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import torch\n",
    "\n",
    "class torchNN(object):\n",
    "\n",
    "    def __init__(self, input_nodes, hidden_nodes, output_nodes, learning_rate=0.5, batch=64):\n",
    "\n",
    "        self.BATCH, self.D_in, self.H, self.D_out = batch, input_nodes, hidden_nodes, output_nodes\n",
    "\n",
    "        self.lr = learning_rate\n",
    "\n",
    "        dtype = torch.float\n",
    "        device = torch.device(\"cpu\")\n",
    "\n",
    "        self.w1 = torch.randn(self.D_in, self.H, device=device, dtype=torch.float64, requires_grad=True)\n",
    "        self.w2 = torch.randn(self.H, self.D_out, device=device, dtype=torch.float64, requires_grad=True)\n",
    "\n",
    "        self.activation_fuc = torch.sigmoid#torch.softmax  使用softmax时要指定dim\n",
    "\n",
    "        self.loss_func = lambda x, y: (x - y).pow(2).sum().mean()\n",
    "\n",
    "    def train(self, data_x, data_y, EPOCHS=500):\n",
    "\n",
    "        # x = torch.from_numpy(data_x[:train_size, :])\n",
    "        # y = torch.from_numpy(data_y[:train_size, :])\n",
    "        x = torch.from_numpy(data_x)\n",
    "        y = torch.from_numpy(data_y)\n",
    "        \n",
    "        for t in range(EPOCHS):\n",
    "            pre = 0\n",
    "            for bt in range(int(x.shape[0] / self.BATCH)):\n",
    "                # Forward pass: compute predicted y using operations on Tensors; these\n",
    "                # are exactly the same operations we used to compute the forward pass using\n",
    "                # Tensors, but we do not need to keep references to intermediate values since\n",
    "                # we are not implementing the backward pass by hand.\n",
    "                y_pred = self.activation_fuc(self.activation_fuc(x[pre:bt * self.BATCH, :] \\\n",
    "                                                                 .mm(self.w1).clamp(min=0)).mm(self.w2))\n",
    "\n",
    "                # Compute and print loss using operations on Tensors.\n",
    "                # Now loss is a Tensor of shape (1,)\n",
    "                # loss.item() gets the scalar value held in the loss.\n",
    "                loss = self.loss_func(y_pred, y[pre:bt * self.BATCH, :])\n",
    "\n",
    "                # Use autograd to compute the backward pass. This call will compute the\n",
    "                # gradient of loss with respect to all Tensors with requires_grad=True.\n",
    "                # After this call w1.grad and w2.grad will be Tensors holding the gradient\n",
    "                # of the loss with respect to w1 and w2 respectively.\n",
    "                loss.backward()\n",
    "\n",
    "                # Manually update weights using gradient descent. Wrap in torch.no_grad()\n",
    "                # because weights have requires_grad=True, but we don't need to track this\n",
    "                # in autograd.\n",
    "                # An alternative way is to operate on weight.data and weight.grad.data.\n",
    "                # Recall that tensor.data gives a tensor that shares the storage with\n",
    "                # tensor, but doesn't track history.\n",
    "                # You can also use torch.optim.SGD to achieve this.\n",
    "                with torch.no_grad():\n",
    "                    self.w1 -= self.lr * self.w1.grad\n",
    "                    self.w2 -= self.lr * self.w2.grad\n",
    "\n",
    "                    # Manually zero the gradients after updating weights\n",
    "                    self.w1.grad.zero_()\n",
    "                    self.w2.grad.zero_()\n",
    "\n",
    "                pre = bt * self.BATCH\n",
    "            if t % 100 == 99:\n",
    "                print(t, loss.item())\n",
    "\n",
    "        pass\n",
    "\n",
    "    def query_class(self, data_x):\n",
    "\n",
    "        y_pred = self.query_prob(data_x)\n",
    "\n",
    "        return y_pred.argmax(dim=1)\n",
    "\n",
    "    def query_prob(self, data_x):\n",
    "        x = torch.from_numpy(data_x)\n",
    "        y = torch.from_numpy(data_y)\n",
    "\n",
    "        y_pred = x.mm(self.w1).clamp(min=0).mm(self.w2)\n",
    "\n",
    "        return y_pred\n",
    "\n",
    "    def cal_score(self, targets, pred):\n",
    "\n",
    "        #print('targets:\\n{0}'.format(targets))\n",
    "        #print('pred:\\n{0}'.format(pred))\n",
    "\n",
    "        return cal_accuration(targets, pred)\n",
    "\n",
    "\n",
    "    \n",
    "'''\n",
    "\n",
    "# device = torch.device(\"cuda:0\") # Uncomment this to run on GPU\n",
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, INPUT_NODES, HIDDEN_NODES, OUTPUT_NODES\n",
    "\n",
    "# Create random Tensors to hold input and outputs.\n",
    "# Setting requires_grad=False indicates that we do not need to compute gradients\n",
    "# with respect to these Tensors during the backward pass.\n",
    "x = torch.from_numpy(data_x[:train_size, :])\n",
    "y = torch.from_numpy(data_y[:train_size, :])\n",
    "#x = torch.randn(N, D_in, device=device, dtype=dtype)\n",
    "#y = torch.randn(N, D_out, device=device, dtype=dtype)\n",
    "\n",
    "# Create random Tensors for weights.\n",
    "# Setting requires_grad=True indicates that we want to compute gradients with\n",
    "# respect to these Tensors during the backward pass.\n",
    "#注意调节dtype类型与x，y一致\n",
    "w1 = torch.randn(D_in, H, device=device, dtype=torch.float64, requires_grad=True)\n",
    "w2 = torch.randn(H, D_out, device=device, dtype=torch.float64, requires_grad=True)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(500):\n",
    "    # Forward pass: compute predicted y using operations on Tensors; these\n",
    "    # are exactly the same operations we used to compute the forward pass using\n",
    "    # Tensors, but we do not need to keep references to intermediate values since\n",
    "    # we are not implementing the backward pass by hand.\n",
    "    y_pred = x.mm(w1).clamp(min=0).mm(w2)\n",
    "\n",
    "    # Compute and print loss using operations on Tensors.\n",
    "    # Now loss is a Tensor of shape (1,)\n",
    "    # loss.item() gets the scalar value held in the loss.\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss.item())\n",
    "\n",
    "    # Use autograd to compute the backward pass. This call will compute the\n",
    "    # gradient of loss with respect to all Tensors with requires_grad=True.\n",
    "    # After this call w1.grad and w2.grad will be Tensors holding the gradient\n",
    "    # of the loss with respect to w1 and w2 respectively.\n",
    "    loss.backward()\n",
    "\n",
    "    # Manually update weights using gradient descent. Wrap in torch.no_grad()\n",
    "    # because weights have requires_grad=True, but we don't need to track this\n",
    "    # in autograd.\n",
    "    # An alternative way is to operate on weight.data and weight.grad.data.\n",
    "    # Recall that tensor.data gives a tensor that shares the storage with\n",
    "    # tensor, but doesn't track history.\n",
    "    # You can also use torch.optim.SGD to achieve this.\n",
    "    with torch.no_grad():\n",
    "        w1 -= learning_rate * w1.grad\n",
    "        w2 -= learning_rate * w2.grad\n",
    "\n",
    "        # Manually zero the gradients after updating weights\n",
    "        w1.grad.zero_()\n",
    "        w2.grad.zero_()\n",
    "        \n",
    "'''\n",
    "\n",
    "print('pass anotation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 112.94060754134237\n",
      "199 12.839746027717098\n",
      "299 2.9455860840117385\n",
      "399 1.5502047001618022\n",
      "499 1.0369493476717038\n",
      "599 0.815565787257189\n",
      "699 0.6220625771328351\n",
      "799 0.5257240953014325\n",
      "899 0.4268887599695928\n",
      "999 0.3593803392154442\n"
     ]
    }
   ],
   "source": [
    "t_nn = torchNN(INPUT_NODES, HIDDEN_NODES, OUTPUT_NODES, 0.01, 100)\n",
    "\n",
    "t_nn.train(data_x[:train_size, :], data_y[:train_size, :], 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "784"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "INPUT_NODES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.5712, -0.9409,  1.0245,  ...,  1.1481, -0.9732, -0.7683],\n",
       "        [ 0.0893, -0.5080, -0.3244,  ...,  0.2482,  0.5742, -1.5725],\n",
       "        [-0.1938,  1.0782,  1.3841,  ..., -1.1190, -0.2287,  0.9928],\n",
       "        ...,\n",
       "        [-0.5748,  1.1317, -0.4716,  ...,  0.7497,  0.7375, -2.4411],\n",
       "        [-0.0032,  0.8854, -0.5419,  ..., -0.9716, -0.6121,  0.6318],\n",
       "        [-1.4669,  0.0612,  1.8497,  ...,  1.5659,  0.3883, -0.4293]],\n",
       "       dtype=torch.float64, requires_grad=True)"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_nn.w1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.7642e+02, -5.3258e+01,  1.7015e+02, -9.8297e+00,  1.7220e+01,\n",
       "         -6.7392e+01,  4.1157e+01,  1.6134e+02, -3.4256e+01, -7.3320e+01],\n",
       "        [-2.5804e+02, -3.5994e+01,  2.1984e+02, -4.1956e+01,  1.0050e+01,\n",
       "          1.8289e+02, -1.0554e+02,  3.4350e+01,  1.0980e+02, -9.0497e+01],\n",
       "        [-1.7783e+02, -1.3282e+02,  5.8818e+01,  4.2387e+01,  6.4154e+01,\n",
       "         -1.1165e+02,  8.7715e+01,  1.4058e+02,  9.6029e+01,  3.4567e+01],\n",
       "        [ 2.0007e+01, -1.1693e+02,  5.1597e+01, -1.5646e+01, -3.5673e+01,\n",
       "          1.1998e+02, -4.1613e+00,  1.5198e+02, -2.7694e+01, -1.4781e+02],\n",
       "        [-1.4635e+02,  4.9652e+01,  2.3465e+01, -3.5986e+00,  1.9863e+01,\n",
       "          4.6024e+01, -1.1523e+01,  4.4960e+01,  5.6759e+01, -7.4955e+01],\n",
       "        [ 1.6551e+01, -1.6748e+02,  3.0366e+02, -1.4884e+02, -7.1778e+01,\n",
       "         -9.8943e+00,  2.3641e+02,  2.0602e+02, -1.9151e+01, -2.3881e+02],\n",
       "        [-1.2839e+02, -4.6448e+01,  3.0525e+02, -3.5362e+01, -1.0262e+02,\n",
       "          2.6968e+01,  1.2211e+02,  2.9145e+01,  6.6916e+01, -1.3741e+02],\n",
       "        [ 1.6186e+02, -6.3285e+01,  6.8227e+00, -1.0354e+02, -1.1575e+02,\n",
       "          2.9687e+02,  8.3632e+00, -5.4971e+01, -1.6513e+01, -1.4274e+02],\n",
       "        [-7.8675e+01, -2.1616e+02, -5.5293e+01,  2.6804e-01,  4.7832e+00,\n",
       "          1.8763e+02, -3.5163e+01,  9.9472e+01,  1.0020e+02, -4.0261e+00],\n",
       "        [-1.6774e+02, -1.1220e+02,  1.3932e+02, -9.9392e+00,  2.4897e+01,\n",
       "         -4.0078e+01,  8.4860e+01,  8.5700e+01,  4.8187e+01,  1.8387e+01]],\n",
       "       dtype=torch.float64, grad_fn=<MmBackward>)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_nn.query_prob(data_x[-10:, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29, 0.29)"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_nn.cal_score(data_y[train_size:, :].argmax(1), t_nn.query_class(data_x[train_size:, :]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(159, 0.17666666666666667)"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_nn.cal_score(data_y[:train_size, :].argmax(1), t_nn.query_class(data_x[:train_size, :]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.01, 0.01, 0.01, ..., 0.99, 0.01, 0.01],\n",
       "       [0.01, 0.01, 0.99, ..., 0.01, 0.01, 0.01],\n",
       "       [0.01, 0.99, 0.01, ..., 0.01, 0.01, 0.01],\n",
       "       ...,\n",
       "       [0.99, 0.01, 0.01, ..., 0.01, 0.01, 0.01],\n",
       "       [0.01, 0.01, 0.01, ..., 0.01, 0.99, 0.01],\n",
       "       [0.01, 0.01, 0.01, ..., 0.01, 0.01, 0.99]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected object of scalar type Double but got scalar type Float for argument #2 'mat2'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-dc9ee049eb05>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected object of scalar type Double but got scalar type Float for argument #2 'mat2'"
     ]
    }
   ],
   "source": [
    "x.mm(w1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "softmax() missing 1 required positional arguments: \"dim\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-51-0a8995ec4c60>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: softmax() missing 1 required positional arguments: \"dim\""
     ]
    }
   ],
   "source": [
    "torch.softmax(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
